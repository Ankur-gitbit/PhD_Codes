{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyM+LwcrCNgYWL4K2OOQ757e",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ankur-gitbit/PhD_Codes/blob/main/AFM_forcespec_data_extractor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DhPryNMEm3Vl"
      },
      "outputs": [],
      "source": [
        "# %% [markdown]\n",
        "# # AFM Force Spectroscopy Data Processor - Fresh Start\n",
        "#\n",
        "# This notebook processes multiple TSV files from AFM force spectroscopy experiments on hydrogels and combines them into an organized Excel file.\n",
        "# **NOTE: This notebook will CLEAR all previously generated files before starting.**\n",
        "#\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 0. CLEAR ALL PREVIOUS FILES (START FRESH)\n",
        "\n",
        "# %%\n",
        "import os\n",
        "import glob\n",
        "import shutil\n",
        "from google.colab import files\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import zipfile\n",
        "import io\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "import seaborn as sns\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "def clear_previous_files():\n",
        "    \"\"\"Remove all previously generated files and directories\"\"\"\n",
        "    print(\"üî¥ CLEARING ALL PREVIOUS FILES...\")\n",
        "\n",
        "    # List of directories and files to remove\n",
        "    targets = [\n",
        "        'uploaded_files',\n",
        "        'AFM_Force_Spectroscopy_Combined.xlsx',\n",
        "        'AFM_Data_Visualization.png',\n",
        "        '*.xlsx',\n",
        "        '*.png',\n",
        "        '*.pdf',\n",
        "        'collagen_*',\n",
        "        'hydrogel_*',\n",
        "        'results_*'\n",
        "    ]\n",
        "\n",
        "    files_removed = 0\n",
        "    directories_removed = 0\n",
        "\n",
        "    # Remove specific files\n",
        "    for file_pattern in ['*.xlsx', '*.png', '*.pdf', 'AFM_*', 'collagen_*', 'hydrogel_*', 'results_*']:\n",
        "        for filepath in glob.glob(file_pattern):\n",
        "            try:\n",
        "                os.remove(filepath)\n",
        "                print(f\"  Removed file: {filepath}\")\n",
        "                files_removed += 1\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "    # Remove directories\n",
        "    for dir_path in ['uploaded_files', 'processed_data', 'output', 'uploaded_files_NEW']:\n",
        "        if os.path.exists(dir_path):\n",
        "            try:\n",
        "                shutil.rmtree(dir_path)\n",
        "                print(f\"  Removed directory: {dir_path}\")\n",
        "                directories_removed += 1\n",
        "            except Exception as e:\n",
        "                pass\n",
        "\n",
        "    # Clear output in Colab\n",
        "    from IPython.display import clear_output\n",
        "    clear_output(wait=True)\n",
        "\n",
        "    print(f\"\\n‚úÖ Cleanup complete!\")\n",
        "    print(f\"   Removed {files_removed} files and {directories_removed} directories\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"You can now upload NEW hydrogel sample data.\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Execute cleanup\n",
        "clear_previous_files()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 1. Install Required Libraries\n",
        "\n",
        "# %%\n",
        "# Install required packages\n",
        "!pip install pandas openpyxl tqdm seaborn -q\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 2. Get Project/Sample Name from User\n",
        "\n",
        "# %%\n",
        "def get_project_name():\n",
        "    \"\"\"Get project/sample name from user to prefix all output files\"\"\"\n",
        "    print(\"üìù ENTER PROJECT/SAMPLE NAME\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"This name will be used as a prefix for all output files.\")\n",
        "    print(\"Examples: 'collagen', 'PEG_10percent', 'alginate_UVcrosslinked'\")\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    project_name = input(\"Enter project name: \").strip()\n",
        "\n",
        "    if not project_name:\n",
        "        project_name = \"hydrogel_AFM\"\n",
        "        print(f\"‚ö†Ô∏è  No name entered. Using default: '{project_name}'\")\n",
        "\n",
        "    # Clean the name for filename compatibility\n",
        "    project_name_clean = \"\".join(c for c in project_name if c.isalnum() or c in ('_', '-')).rstrip()\n",
        "\n",
        "    print(f\"\\n‚úÖ Project name set to: '{project_name_clean}'\")\n",
        "    print(f\"   All output files will start with: {project_name_clean}_*\")\n",
        "\n",
        "    return project_name_clean\n",
        "\n",
        "# Get project name\n",
        "project_name = get_project_name()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 3. Upload Your TSV Files (NEW SAMPLES)\n",
        "\n",
        "# %%\n",
        "def upload_files():\n",
        "    \"\"\"Upload TSV files to Colab - Fresh upload for new samples\"\"\"\n",
        "    print(f\"\\nüì§ UPLOAD TSV FILES FOR '{project_name}' SAMPLES\")\n",
        "    print(\"-\" * 50)\n",
        "    print(\"Please upload your TSV files for the new hydrogel samples.\")\n",
        "    print(\"You can select multiple files at once.\")\n",
        "    print(\"If your files are in a zip archive, please upload the zip file.\")\n",
        "    print(\"\\n‚ö†Ô∏è  Note: All previously uploaded data has been cleared.\")\n",
        "\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"\\n‚ùå No files uploaded. Please run this cell again to upload files.\")\n",
        "        return None\n",
        "\n",
        "    # Create fresh directory with project name\n",
        "    upload_dir = f'./uploaded_files_{project_name}'\n",
        "    if os.path.exists(upload_dir):\n",
        "        shutil.rmtree(upload_dir)\n",
        "    os.makedirs(upload_dir, exist_ok=True)\n",
        "\n",
        "    # Check if a zip file was uploaded\n",
        "    zip_files = [f for f in uploaded.keys() if f.endswith('.zip')]\n",
        "\n",
        "    if zip_files:\n",
        "        print(f\"\\nüì¶ Extracting {len(zip_files)} zip file(s)...\")\n",
        "        for zip_file in zip_files:\n",
        "            with zipfile.ZipFile(io.BytesIO(uploaded[zip_file]), 'r') as z:\n",
        "                z.extractall(upload_dir)\n",
        "        print(\"‚úÖ Extraction complete!\")\n",
        "    else:\n",
        "        # Save individual files\n",
        "        for filename, content in uploaded.items():\n",
        "            with open(os.path.join(upload_dir, filename), 'wb') as f:\n",
        "                f.write(content)\n",
        "        print(f\"\\n‚úÖ Saved {len(uploaded)} file(s) to {upload_dir}/\")\n",
        "\n",
        "    return upload_dir\n",
        "\n",
        "# Upload NEW files\n",
        "upload_dir = upload_files()\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 4. Set Output Directory (Same as Input Directory)\n",
        "\n",
        "# %%\n",
        "# Set the output directory to be the same as the input directory\n",
        "if upload_dir:\n",
        "    output_dir = upload_dir\n",
        "    print(f\"\\nüìÅ OUTPUT DIRECTORY SET TO INPUT DIRECTORY:\")\n",
        "    print(f\"   {output_dir}\")\n",
        "    print(\"=\" * 60)\n",
        "else:\n",
        "    output_dir = \".\"\n",
        "    print(\"‚ö†Ô∏è  Using current directory as output directory\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 5. Find and List TSV Files (NEW DATA)\n",
        "\n",
        "# %%\n",
        "def find_tsv_files(directory):\n",
        "    \"\"\"Find all TSV files in the directory\"\"\"\n",
        "    tsv_files = glob.glob(os.path.join(directory, '*.tsv'))\n",
        "    txt_files = glob.glob(os.path.join(directory, '*.txt'))\n",
        "\n",
        "    all_files = tsv_files + txt_files\n",
        "\n",
        "    if not all_files:\n",
        "        print(f\"‚ö†Ô∏è  No TSV or TXT files found in {directory}\")\n",
        "        print(\"üîç Checking subdirectories...\")\n",
        "        all_files = glob.glob(os.path.join(directory, '**', '*.tsv'), recursive=True)\n",
        "        all_files += glob.glob(os.path.join(directory, '**', '*.txt'), recursive=True)\n",
        "\n",
        "    return sorted(all_files)\n",
        "\n",
        "if upload_dir:\n",
        "    tsv_files = find_tsv_files(upload_dir)\n",
        "\n",
        "    if tsv_files:\n",
        "        print(f\"‚úÖ Found {len(tsv_files)} TSV/TXT files for '{project_name}':\")\n",
        "        print(\"-\" * 60)\n",
        "        for i, filepath in enumerate(tsv_files[:15], 1):\n",
        "            filename = os.path.basename(filepath)\n",
        "            size_kb = os.path.getsize(filepath) / 1024\n",
        "            print(f\"{i:3d}. {filename:<50} ({size_kb:.1f} KB)\")\n",
        "\n",
        "        if len(tsv_files) > 15:\n",
        "            print(f\"... and {len(tsv_files) - 15} more files\")\n",
        "\n",
        "        print(\"-\" * 60)\n",
        "\n",
        "        # Show file details\n",
        "        print(\"\\nüìã FILE DETAILS:\")\n",
        "        sample_names = set()\n",
        "        for filepath in tsv_files[:5]:  # Show first 5 files as sample\n",
        "            with open(filepath, 'r', encoding='utf-8', errors='ignore') as f:\n",
        "                first_line = f.readline().strip()\n",
        "                if first_line:\n",
        "                    sample_names.add(os.path.basename(filepath).split('_')[0] if '_' in os.path.basename(filepath) else os.path.splitext(os.path.basename(filepath))[0])\n",
        "\n",
        "        if sample_names:\n",
        "            print(f\"Sample names detected: {', '.join(list(sample_names)[:5])}\")\n",
        "            if len(sample_names) > 5:\n",
        "                print(f\"... and {len(sample_names) - 5} more\")\n",
        "    else:\n",
        "        print(\"‚ùå No TSV files found. Please check your upload.\")\n",
        "        print(\"   Make sure files have .tsv or .txt extensions.\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 6. Data Loading and Processing Functions\n",
        "\n",
        "# %%\n",
        "def load_tsv_file(filepath, sep='\\t'):\n",
        "    \"\"\"Load a TSV file with proper error handling, trying multiple encodings and separators\"\"\"\n",
        "    encodings_to_try = ['utf-8', 'latin-1', 'cp1252'] # Added more common encodings\n",
        "\n",
        "    for encoding in encodings_to_try:\n",
        "        try:\n",
        "            df = pd.read_csv(filepath, sep=sep, encoding=encoding)\n",
        "\n",
        "            # If the file has only one column, try different separators\n",
        "            if df.shape[1] == 1:\n",
        "                print(f\"‚ö†Ô∏è  File {os.path.basename(filepath)} has only 1 column with {encoding} encoding. Trying different separators...\")\n",
        "                # Try comma separator\n",
        "                df_comma = pd.read_csv(filepath, sep=',', encoding=encoding)\n",
        "                if df_comma.shape[1] > 1:\n",
        "                    print(f\"  ‚úÖ Successfully read with comma separator using {encoding}\")\n",
        "                    df = df_comma\n",
        "                else:\n",
        "                    # Try semicolon\n",
        "                    df_semicolon = pd.read_csv(filepath, sep=';', encoding=encoding)\n",
        "                    if df_semicolon.shape[1] > 1:\n",
        "                        print(f\"  ‚úÖ Successfully read with semicolon separator using {encoding}\")\n",
        "                        df = df_semicolon\n",
        "                    else:\n",
        "                        # Try whitespace\n",
        "                        df_whitespace = pd.read_csv(filepath, sep='\\s+', encoding=encoding, engine='python')\n",
        "                        if df_whitespace.shape[1] > 1:\n",
        "                            print(f\"  ‚úÖ Successfully read with whitespace separator using {encoding}\")\n",
        "                            df = df_whitespace\n",
        "                        else:\n",
        "                            # If still one column, this encoding/separator combination failed\n",
        "                            continue # Try next encoding\n",
        "\n",
        "            # If we reached here, df was successfully read (potentially with a different separator)\n",
        "            df.columns = df.columns.str.strip() # Clean column names\n",
        "            return df\n",
        "\n",
        "        except UnicodeDecodeError:\n",
        "            # print(f\"Trying {encoding} failed for {os.path.basename(filepath)}. Trying next encoding...\")\n",
        "            continue # Try next encoding\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Non-decoding error reading {os.path.basename(filepath)} with {encoding}: {e}\")\n",
        "            break # Break from encoding loop if it's a pandas parsing error or other issue\n",
        "\n",
        "    print(f\"‚ùå Failed to read {os.path.basename(filepath)} with any attempted encoding or separator.\")\n",
        "    return None\n",
        "\n",
        "def extract_sample_info(filename):\n",
        "    \"\"\"Extract sample information from filename\"\"\"\n",
        "    # Remove extension\n",
        "    name = os.path.splitext(filename)[0]\n",
        "\n",
        "    # Try to extract information from filename patterns\n",
        "    info = {\n",
        "        'filename': filename,\n",
        "        'sample_name': name,\n",
        "        'date': '',\n",
        "        'condition': '',\n",
        "        'replicate': '',\n",
        "        'hydrogel_type': '',\n",
        "        'concentration': '',\n",
        "        'crosslinking': ''\n",
        "    }\n",
        "\n",
        "    # Common patterns in hydrogel filenames\n",
        "    import re\n",
        "\n",
        "    # Look for hydrogel type patterns\n",
        "    hydrogel_patterns = [\n",
        "        r'(PEG[-\\s_]?\\w*)', r'(alginate)', r'(collagen)', r'(gelatin)',\n",
        "        r'(HA|hyaluronic)', r'(chitosan)', r'(agarose)', r'(PAAm|polyacrylamide)',\n",
        "        r'(PVA)', r'(PNIPAM)', r'(PHEMA)'\n",
        "    ]\n",
        "\n",
        "    for pattern in hydrogel_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            info['hydrogel_type'] = match.group(1)\n",
        "            break\n",
        "\n",
        "    # Look for concentration patterns\n",
        "    conc_patterns = [\n",
        "        r'(\\d+[\\.]?\\d*)[\\s_]?%', r'(\\d+[\\.]?\\d*)[\\s_]?mg/ml',\n",
        "        r'(\\d+[\\.]?\\d*)[\\s_]?mM', r'(\\d+[\\.]?\\d*)[\\s_]?wt'\n",
        "    ]\n",
        "\n",
        "    for pattern in conc_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            info['concentration'] = match.group(1)\n",
        "            break\n",
        "\n",
        "    # Look for crosslinking patterns\n",
        "    xlink_patterns = [\n",
        "        r'(UV)', r'(thermal)', r'(chemical)', r'(enzymatic)',\n",
        "        r'(CaCl2|calcium)', r'(EDC)', r'(NHS)', r'(genipin)'\n",
        "    ]\n",
        "\n",
        "    for pattern in xlink_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            info['crosslinking'] = match.group(1)\n",
        "            break\n",
        "\n",
        "    # Look for date patterns\n",
        "    date_patterns = [\n",
        "        r'\\d{4}-\\d{2}-\\d{2}',\n",
        "        r'\\d{8}',\n",
        "        r'\\d{2}-\\d{2}-\\d{4}'\n",
        "    ]\n",
        "\n",
        "    for pattern in date_patterns:\n",
        "        match = re.search(pattern, name)\n",
        "        if match:\n",
        "            info['date'] = match.group()\n",
        "            break\n",
        "\n",
        "    # Look for replicate information\n",
        "    rep_patterns = [\n",
        "        r'_rep(\\d+)', r'_r(\\d+)', r'_(\\d+)$', r'[_-]([a-zA-Z])$',\n",
        "        r'replicate[\\s_-]?(\\d+)', r'sample[\\s_-]?(\\d+)'\n",
        "    ]\n",
        "\n",
        "    for pattern in rep_patterns:\n",
        "        match = re.search(pattern, name, re.IGNORECASE)\n",
        "        if match:\n",
        "            info['replicate'] = match.group(1)\n",
        "            break\n",
        "\n",
        "    # Look for condition (e.g., pH, temperature)\n",
        "    if 'ph' in name.lower():\n",
        "        ph_match = re.search(r'[pP][hH][_-]?(\\d+\\.?\\d*)', name)\n",
        "        if ph_match:\n",
        "            info['condition'] = f\"pH {ph_match.group(1)}\"\n",
        "    elif 'temp' in name.lower() or '¬∞C' in name:\n",
        "        temp_match = re.search(r'(\\d+)[\\s_]?¬∞?C', name, re.IGNORECASE)\n",
        "        if temp_match:\n",
        "            info['condition'] = f\"{temp_match.group(1)}¬∞C\"\n",
        "\n",
        "    return info\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 7. Process All Files and Create Fresh Excel Workbook (Saved in Input Directory)\n",
        "\n",
        "# %%\n",
        "def process_all_files(tsv_files, project_name, output_directory):\n",
        "    \"\"\"Process all TSV files and create a fresh Excel workbook\"\"\"\n",
        "\n",
        "    if not tsv_files:\n",
        "        print(\"‚ùå No files to process!\")\n",
        "        return None\n",
        "\n",
        "    print(f\"\\nüîÑ PROCESSING {len(tsv_files)} FILES FOR '{project_name}'...\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    summary_data = []\n",
        "    all_data_combined_dfs = [] # Renamed to clearly indicate it holds processed DataFrames\n",
        "\n",
        "    processed_count = 0\n",
        "    error_count = 0\n",
        "\n",
        "    for filepath in tqdm(tsv_files, desc=\"Processing\", unit=\"file\"):\n",
        "        filename = os.path.basename(filepath)\n",
        "        sample_info = extract_sample_info(filename)\n",
        "        df = load_tsv_file(filepath)\n",
        "\n",
        "        if df is not None and not df.empty:\n",
        "            processed_count += 1\n",
        "\n",
        "            df['Source_Filename'] = filename\n",
        "\n",
        "            for key, value in sample_info.items():\n",
        "                if key != 'filename':\n",
        "                    df[f'Sample_{key}'] = value\n",
        "\n",
        "            summary_stats = {\n",
        "                'Filename': filename,\n",
        "                'Sample_Name': sample_info['sample_name'],\n",
        "                'Hydrogel_Type': sample_info['hydrogel_type'],\n",
        "                'Concentration': sample_info['concentration'],\n",
        "                'Crosslinking': sample_info['crosslinking'],\n",
        "                'Date': sample_info['date'],\n",
        "                'Condition': sample_info['condition'],\n",
        "                'Replicate': sample_info['replicate'],\n",
        "                'Total_Curves': len(df),\n",
        "                'Youngs_Modulus_Mean_Pa': df.get('Young\\'s Modulus [Pa]', pd.Series([np.nan])).mean() if 'Young\\'s Modulus [Pa]' in df.columns else np.nan,\n",
        "                'Youngs_Modulus_Std_Pa': df.get('Young\\'s Modulus [Pa]', pd.Series([np.nan])).std() if 'Young\\'s Modulus [Pa]' in df.columns else np.nan,\n",
        "                'Youngs_Modulus_Mean_kPa': (df.get('Young\\'s Modulus [Pa]', pd.Series([np.nan])).mean() / 1000) if 'Young\\'s Modulus [Pa]' in df.columns else np.nan,\n",
        "                'Adhesion_Mean_N': df.get('Adhesion', pd.Series([np.nan])).mean() if 'Adhesion' in df.columns else np.nan,\n",
        "                'Adhesion_Std_N': df.get('Adhesion', pd.Series([np.nan])).std() if 'Adhesion' in df.columns else np.nan,\n",
        "                'Slope_Mean_N/m': df.get('Slope [N/m]', pd.Series([np.nan])).mean() if 'Slope [N/m]' in df.columns else np.nan,\n",
        "                'Height_Mean_m': df.get('Interpolated Height [m]', pd.Series([np.nan])).mean() if 'Interpolated Height [m]' in df.columns else np.nan,\n",
        "            }\n",
        "\n",
        "            if 'Contact Point [m]' in df.columns:\n",
        "                summary_stats['Contact_Point_Mean_m'] = df['Contact Point [m]'].mean()\n",
        "\n",
        "            summary_data.append(summary_stats)\n",
        "            all_data_combined_dfs.append(df) # Append the processed dataframe\n",
        "        else:\n",
        "            error_count += 1\n",
        "            print(f\"‚ö†Ô∏è  Skipped {filename} - could not read or empty file\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(f\"‚úÖ PROCESSING COMPLETE FOR '{project_name}'!\")\n",
        "    print(f\"   Processed: {processed_count} files successfully\")\n",
        "    print(f\"   Failed: {error_count} files\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # --- Crucial change: Only create Excel writer if there's data to write --- BEGIN\n",
        "    if processed_count == 0:\n",
        "        print(\"‚ùå No files were successfully processed. No Excel file will be generated.\")\n",
        "        return None\n",
        "    # --- Crucial change: Only create Excel writer if there's data to write --- END\n",
        "\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_filename = f\"{project_name}_AFM_Results_{timestamp}.xlsx\"\n",
        "    output_path = os.path.join(output_directory, output_filename)\n",
        "\n",
        "    if os.path.exists(output_path):\n",
        "        os.remove(output_path)\n",
        "        print(f\"üóëÔ∏è  Removed previous output file: {output_path}\")\n",
        "\n",
        "    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:\n",
        "        # Write individual sheets for each processed dataframe\n",
        "        for i, df_to_write in enumerate(all_data_combined_dfs):\n",
        "            sample_name = df_to_write['Sample_sample_name'].iloc[0] if 'Sample_sample_name' in df_to_write.columns else f\"UnknownSample_{i+1}\"\n",
        "            sheet_name = sample_name[:31] # Excel limit for sheet names\n",
        "            invalid_chars = '[]:*?/\\\\'\n",
        "            for char in invalid_chars:\n",
        "                sheet_name = sheet_name.replace(char, '_')\n",
        "            if not sheet_name:\n",
        "                sheet_name = f\"Data_{i+1}\"\n",
        "\n",
        "            try:\n",
        "                df_to_write.to_excel(writer, sheet_name=sheet_name, index=False)\n",
        "            except Exception as e:\n",
        "                sheet_name_unique = f\"{sheet_name[:25]}_{i+1}\"\n",
        "                try:\n",
        "                    df_to_write.to_excel(writer, sheet_name=sheet_name_unique, index=False)\n",
        "                except:\n",
        "                    sheet_name_unique = f\"Data_{i+1}_Error\" # Fallback\n",
        "                    df_to_write.to_excel(writer, sheet_name=sheet_name_unique, index=False)\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        summary_df.to_excel(writer, sheet_name='Summary_Statistics', index=False)\n",
        "\n",
        "        overall_stats = {\n",
        "            'Statistic': ['Project Name', 'Total Files', 'Processed Files', 'Failed Files', 'Total Curves',\n",
        "                        'Avg Youngs Modulus (kPa)', 'Avg Adhesion (nN)', 'Output Directory'],\n",
        "            'Value': [\n",
        "                project_name,\n",
        "                len(tsv_files),\n",
        "                processed_count,\n",
        "                error_count,\n",
        "                summary_df['Total_Curves'].sum(),\n",
        "                summary_df['Youngs_Modulus_Mean_kPa'].mean(),\n",
        "                summary_df['Adhesion_Mean_N'].mean() * 1e9,  # Convert to nN\n",
        "                output_directory\n",
        "            ]\n",
        "        }\n",
        "        overall_df = pd.DataFrame(overall_stats)\n",
        "        overall_df.to_excel(writer, sheet_name='Overall_Stats', index=False)\n",
        "\n",
        "        combined_df = pd.concat(all_data_combined_dfs, ignore_index=True)\n",
        "        combined_df.to_excel(writer, sheet_name='All_Data_Combined', index=False)\n",
        "\n",
        "        if not summary_df.empty: # Ensure summary_df is not empty before attempting analysis\n",
        "            analysis_df = summary_df.copy()\n",
        "\n",
        "            grouping_fields = []\n",
        "            if analysis_df['Hydrogel_Type'].notna().any() and analysis_df['Hydrogel_Type'].nunique() > 1:\n",
        "                grouping_fields.append('Hydrogel_Type')\n",
        "            if analysis_df['Concentration'].notna().any() and analysis_df['Concentration'].nunique() > 1:\n",
        "                grouping_fields.append('Concentration')\n",
        "            if analysis_df['Condition'].notna().any() and analysis_df['Condition'].nunique() > 1:\n",
        "                grouping_fields.append('Condition')\n",
        "\n",
        "            if grouping_fields:\n",
        "                analysis_stats = []\n",
        "                for field in grouping_fields:\n",
        "                    for group_value, group in analysis_df.groupby(field):\n",
        "                        if pd.notna(group_value):  # Skip NaN groups\n",
        "                            group_stats = {\n",
        "                                'Grouping_Field': field,\n",
        "                                'Group_Value': group_value,\n",
        "                                'Number_of_Samples': len(group),\n",
        "                                'Number_of_Curves': group['Total_Curves'].sum(),\n",
        "                                'Youngs_Modulus_Mean_kPa': group['Youngs_Modulus_Mean_kPa'].mean(),\n",
        "                                'Youngs_Modulus_SEM_kPa': group['Youngs_Modulus_Mean_kPa'].sem(),\n",
        "                                'Adhesion_Mean_nN': (group['Adhesion_Mean_N'].mean() * 1e9) if group['Adhesion_Mean_N'].notna().any() else np.nan,\n",
        "                                'Adhesion_SEM_nN': (group['Adhesion_Mean_N'].sem() * 1e9) if group['Adhesion_Mean_N'].notna().any() else np.nan\n",
        "                            }\n",
        "                            analysis_stats.append(group_stats)\n",
        "\n",
        "                if analysis_stats:\n",
        "                    analysis_df_grouped = pd.DataFrame(analysis_stats)\n",
        "                    analysis_df_grouped.to_excel(writer, sheet_name='Group_Analysis', index=False)\n",
        "\n",
        "    print(f\"   Output file: {output_path}\")\n",
        "\n",
        "    # Show file size\n",
        "    if os.path.exists(output_path):\n",
        "        file_size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
        "        print(f\"   File size: {file_size_mb:.2f} MB\")\n",
        "\n",
        "    return output_path\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 8. Execute Processing (NEW DATA)\n",
        "\n",
        "# %%\n",
        "if 'tsv_files' in locals() and tsv_files:\n",
        "    output_file = process_all_files(tsv_files, project_name, output_dir)\n",
        "\n",
        "    if output_file:\n",
        "        print(f\"\\nüìä EXCEL FILE CONTENTS:\")\n",
        "        print(\"-\" * 40)\n",
        "        print(\"1. Individual sheets for each TSV file\")\n",
        "        print(\"2. 'Summary_Statistics' sheet with per-file statistics\")\n",
        "        print(\"3. 'Overall_Stats' sheet with summary metrics\")\n",
        "        print(\"4. 'All_Data_Combined' sheet with all data merged\")\n",
        "        print(\"5. 'Group_Analysis' sheet (if groups identified)\")\n",
        "        print(\"-\" * 40)\n",
        "\n",
        "        # Download the file\n",
        "        print(f\"\\nüì• DOWNLOADING RESULTS FILE...\")\n",
        "        files.download(output_file)\n",
        "else:\n",
        "    print(\"‚ùå No files to process. Please upload files in section 2.\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 9. Preview New Data\n",
        "\n",
        "# %%\n",
        "def preview_data(filepath, n_rows=5):\n",
        "    \"\"\"Preview data from a TSV file\"\"\"\n",
        "    df = load_tsv_file(filepath)\n",
        "    if df is not None:\n",
        "        print(f\"üìÑ File: {os.path.basename(filepath)}\")\n",
        "        print(f\"üìê Shape: {df.shape} (rows √ó columns)\")\n",
        "        print(f\"üìã Columns ({len(df.columns)}):\")\n",
        "\n",
        "        # Group columns by type if possible\n",
        "        force_cols = [col for col in df.columns if 'force' in col.lower() or 'adhesion' in col.lower() or '[N]' in col]\n",
        "        mod_cols = [col for col in df.columns if 'modulus' in col.lower() or '[Pa]' in col]\n",
        "        height_cols = [col for col in df.columns if 'height' in col.lower() or '[m]' in col]\n",
        "        slope_cols = [col for col in df.columns if 'slope' in col.lower()]\n",
        "        other_cols = [col for col in df.columns if col not in force_cols + mod_cols + height_cols + slope_cols]\n",
        "\n",
        "        if force_cols:\n",
        "            print(f\"   Force-related: {', '.join(force_cols[:3])}\")\n",
        "            if len(force_cols) > 3:\n",
        "                print(f\"                 ... and {len(force_cols)-3} more\")\n",
        "        if mod_cols:\n",
        "            print(f\"   Modulus: {', '.join(mod_cols)}\")\n",
        "        if height_cols:\n",
        "            print(f\"   Height/Position: {', '.join(height_cols[:2])}\")\n",
        "            if len(height_cols) > 2:\n",
        "                print(f\"                   ... and {len(height_cols)-2} more\")\n",
        "\n",
        "        print(f\"\\nüìà First {n_rows} rows:\")\n",
        "        print(df.head(n_rows))\n",
        "        return df\n",
        "    return None\n",
        "\n",
        "# Optional: Preview the first file\n",
        "if 'tsv_files' in locals() and tsv_files:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üëÅÔ∏è  PREVIEW OF FIRST FILE:\")\n",
        "    print(\"=\" * 60)\n",
        "    preview_df = preview_data(tsv_files[0])\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 10. Create INDIVIDUAL Visualization Plots (Saved in Input Directory)\n",
        "\n",
        "# %%\n",
        "def create_individual_plots(output_file, project_name, output_directory):\n",
        "    \"\"\"Create 6 individual visualization plots for hydrogel characterization\"\"\"\n",
        "    try:\n",
        "        # Read the combined data\n",
        "        combined_df = pd.read_excel(output_file, sheet_name='All_Data_Combined')\n",
        "\n",
        "        # Set style for better looking plots\n",
        "        plt.style.use('seaborn-v0_8-darkgrid')\n",
        "\n",
        "        # Get timestamp for filenames\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        print(f\"\\nüìä CREATING INDIVIDUAL PLOTS FOR '{project_name}'...\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        plot_files = []  # Store all plot file paths\n",
        "\n",
        "        # Plot 1: Young's Modulus distribution (in kPa)\n",
        "        if 'Young\\'s Modulus [Pa]' in combined_df.columns:\n",
        "            print(\"1. Creating Young's Modulus distribution plot...\")\n",
        "            youngs_mod_kpa = combined_df['Young\\'s Modulus [Pa]'].dropna() / 1000\n",
        "\n",
        "            fig1, ax1 = plt.subplots(figsize=(10, 6))\n",
        "            n, bins, patches = ax1.hist(youngs_mod_kpa, bins=30, alpha=0.7, color='steelblue',\n",
        "                                       edgecolor='black', linewidth=0.5)\n",
        "            mean_val = youngs_mod_kpa.mean()\n",
        "            std_val = youngs_mod_kpa.std()\n",
        "\n",
        "            ax1.axvline(mean_val, color='red', linestyle='--', linewidth=2,\n",
        "                       label=f'Mean: {mean_val:.1f} kPa')\n",
        "            ax1.axvline(mean_val + std_val, color='orange', linestyle=':', linewidth=1.5)\n",
        "            ax1.axvline(mean_val - std_val, color='orange', linestyle=':', linewidth=1.5,\n",
        "                       label=f'\\u00b11 SD: {std_val:.1f} kPa')\n",
        "\n",
        "            ax1.set_xlabel('Young\\'s Modulus (kPa)', fontsize=12)\n",
        "            ax1.set_ylabel('Frequency', fontsize=12)\n",
        "            ax1.set_title(f'{project_name} - Young\\'s Modulus Distribution\\n'\n",
        "                         f'N = {len(youngs_mod_kpa)} curves | Mean \\u00b1 SD: {mean_val:.1f} \\u00b1 {std_val:.1f} kPa',\n",
        "                         fontsize=14, fontweight='bold')\n",
        "            ax1.grid(True, alpha=0.3)\n",
        "            ax1.legend()\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save individual plot to output directory\n",
        "            plot1_filename = f\"{project_name}_Youngs_Modulus_Distribution_{timestamp}.png\"\n",
        "            plot1_path = os.path.join(output_directory, plot1_filename)\n",
        "            plt.savefig(plot1_path, dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            print(f\"   ‚úÖ Saved as: {plot1_path}\")\n",
        "            plot_files.append(plot1_path)\n",
        "\n",
        "        # Plot 2: Adhesion distribution (in nN)\n",
        "        if 'Adhesion' in combined_df.columns:\n",
        "            print(\"\\n2. Creating Adhesion force distribution plot...\")\n",
        "            adhesion_nN = combined_df['Adhesion'].dropna() * 1e9  # Convert to nN\n",
        "\n",
        "            fig2, ax2 = plt.subplots(figsize=(10, 6))\n",
        "            n, bins, patches = ax2.hist(adhesion_nN, bins=30, alpha=0.7, color='forestgreen',\n",
        "                                       edgecolor='black', linewidth=0.5)\n",
        "            mean_val = adhesion_nN.mean()\n",
        "            std_val = adhesion_nN.std()\n",
        "\n",
        "            ax2.axvline(mean_val, color='red', linestyle='--', linewidth=2,\n",
        "                       label=f'Mean: {mean_val:.1f} nN')\n",
        "            ax2.axvline(mean_val + std_val, color='orange', linestyle=':', linewidth=1.5)\n",
        "            ax2.axvline(mean_val - std_val, color='orange', linestyle=':', linewidth=1.5,\n",
        "                       label=f'\\u00b11 SD: {std_val:.1f} nN')\n",
        "\n",
        "            ax2.set_xlabel('Adhesion Force (nN)', fontsize=12)\n",
        "            ax2.set_ylabel('Frequency', fontsize=12)\n",
        "            ax2.set_title(f'{project_name} - Adhesion Force Distribution\\n'\n",
        "                         f'N = {len(adhesion_nN)} curves | Mean \\u00b1 SD: {mean_val:.1f} \\u00b1 {std_val:.1f} nN',\n",
        "                         fontsize=14, fontweight='bold')\n",
        "            ax2.grid(True, alpha=0.3)\n",
        "            ax2.legend()\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save individual plot to output directory\n",
        "            plot2_filename = f\"{project_name}_Adhesion_Distribution_{timestamp}.png\"\n",
        "            plot2_path = os.path.join(output_directory, plot2_filename)\n",
        "            plt.savefig(plot2_path, dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            print(f\"   ‚úÖ Saved as: {plot2_path}\")\n",
        "            plot_files.append(plot2_path)\n",
        "\n",
        "        # Plot 3: Young's Modulus vs Adhesion scatter plot\n",
        "        if 'Young\\'s Modulus [Pa]' in combined_df.columns and 'Adhesion' in combined_df.columns:\n",
        "            print(\"\\n3. Creating Modulus vs Adhesion correlation plot...\")\n",
        "            youngs_mod_kpa = combined_df['Young\\'s Modulus [Pa]'].dropna() / 1000\n",
        "            adhesion_nN = combined_df['Adhesion'].dropna() * 1e9\n",
        "\n",
        "            # Align data lengths\n",
        "            min_len = min(len(youngs_mod_kpa), len(adhesion_nN))\n",
        "            if min_len > 0:\n",
        "                youngs_mod_kpa_aligned = youngs_mod_kpa.iloc[:min_len]\n",
        "                adhesion_nN_aligned = adhesion_nN.iloc[:min_len]\n",
        "\n",
        "                fig3, ax3 = plt.subplots(figsize=(10, 6))\n",
        "                scatter = ax3.scatter(youngs_mod_kpa_aligned, adhesion_nN_aligned,\n",
        "                                    alpha=0.6, s=50, c='purple', edgecolor='black', linewidth=0.5)\n",
        "\n",
        "                # Add correlation line if sufficient points\n",
        "                if len(youngs_mod_kpa_aligned) > 1:\n",
        "                    z = np.polyfit(youngs_mod_kpa_aligned, adhesion_nN_aligned, 1)\n",
        "                    p = np.poly1d(z)\n",
        "                    ax3.plot(youngs_mod_kpa_aligned, p(youngs_mod_kpa_aligned),\n",
        "                            \"r--\", alpha=0.8, linewidth=2,\n",
        "                            label=f'Linear fit: y = {z[0]:.3f}x + {z[1]:.1f}')\n",
        "\n",
        "                ax3.set_xlabel('Young\\'s Modulus (kPa)', fontsize=12)\n",
        "                ax3.set_ylabel('Adhesion Force (nN)', fontsize=12)\n",
        "                ax3.set_title(f'{project_name} - Modulus vs Adhesion Correlation\\n'\n",
        "                             f'N = {min_len} data points',\n",
        "                             fontsize=14, fontweight='bold')\n",
        "                ax3.grid(True, alpha=0.3)\n",
        "                ax3.legend()\n",
        "                plt.tight_layout()\n",
        "\n",
        "                # Save individual plot to output directory\n",
        "                plot3_filename = f\"{project_name}_Modulus_vs_Adhesion_{timestamp}.png\"\n",
        "                plot3_path = os.path.join(output_directory, plot3_filename)\n",
        "                plt.savefig(plot3_path, dpi=300, bbox_inches='tight')\n",
        "                plt.show()\n",
        "                print(f\"   ‚úÖ Saved as: {plot3_path}\")\n",
        "                plot_files.append(plot3_path)\n",
        "\n",
        "        # Plot 4: Box plot of Young's Modulus by hydrogel type\n",
        "        if 'Sample_hydrogel_type' in combined_df.columns and 'Young\\'s Modulus [Pa]' in combined_df.columns:\n",
        "            print(\"\\n4. Creating Modulus by Hydrogel Type box plot...\")\n",
        "            hydrogel_types = combined_df['Sample_hydrogel_type'].dropna().unique()\n",
        "\n",
        "            if len(hydrogel_types) > 0:\n",
        "                data_to_plot = []\n",
        "                labels = []\n",
        "\n",
        "                for h_type in hydrogel_types[:8]:  # Limit to 8 types for clarity\n",
        "                    mod_data = combined_df[combined_df['Sample_hydrogel_type'] == h_type]['Young\\'s Modulus [Pa]'].dropna() / 1000\n",
        "                    if len(mod_data) > 0:\n",
        "                        data_to_plot.append(mod_data)\n",
        "                        labels.append(f\"{h_type[:15]}\" if len(h_type) > 15 else h_type)\n",
        "\n",
        "                if data_to_plot:\n",
        "                    fig4, ax4 = plt.subplots(figsize=(12, 7))\n",
        "                    box = ax4.boxplot(data_to_plot, patch_artist=True, showmeans=True,\n",
        "                                     meanline=True, showfliers=True,\n",
        "                                     medianprops=dict(color='black', linewidth=2),\n",
        "                                     meanprops=dict(color='red', linestyle='--', linewidth=2))\n",
        "\n",
        "                    # Color the boxes\n",
        "                    colors = plt.cm.Set3(np.linspace(0, 1, len(data_to_plot)))\n",
        "                    for patch, color in zip(box['boxes'], colors):\n",
        "                        patch.set_facecolor(color)\n",
        "\n",
        "                    ax4.set_xticklabels(labels, rotation=45, ha='right', fontsize=11)\n",
        "                    ax4.set_ylabel('Young\\'s Modulus (kPa)', fontsize=12)\n",
        "                    ax4.set_title(f'{project_name} - Young\\'s Modulus by Hydrogel Type\\n'\n",
        "                                 f'Red dashed line = mean | Black line = median',\n",
        "                                 fontsize=14, fontweight='bold')\n",
        "                    ax4.grid(True, alpha=0.3, axis='y')\n",
        "                    plt.tight_layout()\n",
        "\n",
        "                    # Save individual plot to output directory\n",
        "                    plot4_filename = f\"{project_name}_Modulus_by_Type_{timestamp}.png\"\n",
        "                    plot4_path = os.path.join(output_directory, plot4_filename)\n",
        "                    plt.savefig(plot4_path, dpi=300, bbox_inches='tight')\n",
        "                    plt.show()\n",
        "                    print(f\"   ‚úÖ Saved as: {plot4_path}\")\n",
        "                    plot_files.append(plot4_path)\n",
        "\n",
        "        # Plot 5: Contact point distribution\n",
        "        print(\"\\n5. Creating Contact Point distribution plot...\")\n",
        "        contact_cols = [col for col in combined_df.columns if 'contact' in col.lower() and ('[m]' in col or 'point' in col.lower())]\n",
        "\n",
        "        if contact_cols:\n",
        "            contact_data = combined_df[contact_cols[0]].dropna() * 1e6  # Convert to ¬µm\n",
        "            mean_val = contact_data.mean()\n",
        "            std_val = contact_data.std()\n",
        "\n",
        "            fig5, ax5 = plt.subplots(figsize=(10, 6))\n",
        "            n, bins, patches = ax5.hist(contact_data, bins=30, alpha=0.7, color='darkorange',\n",
        "                                       edgecolor='black', linewidth=0.5)\n",
        "\n",
        "            ax5.axvline(mean_val, color='red', linestyle='--', linewidth=2,\n",
        "                       label=f'Mean: {mean_val:.1f} ¬µm')\n",
        "            ax5.axvline(mean_val + std_val, color='orange', linestyle=':', linewidth=1.5)\n",
        "            ax5.axvline(mean_val - std_val, color='orange', linestyle=':', linewidth=1.5,\n",
        "                       label=f'\\u00b11 SD: {std_val:.1f} ¬µm')\n",
        "\n",
        "            ax5.set_xlabel('Contact Point (¬µm)', fontsize=12)\n",
        "            ax5.set_ylabel('Frequency', fontsize=12)\n",
        "            ax5.set_title(f'{project_name} - Contact Point Distribution\\n'\n",
        "                         f'N = {len(contact_data)} curves | Mean \\u00b1 SD: {mean_val:.1f} \\u00b1 {std_val:.1f} ¬µm',\n",
        "                         fontsize=14, fontweight='bold')\n",
        "            ax5.grid(True, alpha=0.3)\n",
        "            ax5.legend()\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save individual plot to output directory\n",
        "            plot5_filename = f\"{project_name}_Contact_Point_Distribution_{timestamp}.png\"\n",
        "            plot5_path = os.path.join(output_directory, plot5_filename)\n",
        "            plt.savefig(plot5_path, dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            print(f\"   ‚úÖ Saved as: {plot5_path}\")\n",
        "            plot_files.append(plot5_path)\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è  Contact point data not found in the dataset\")\n",
        "\n",
        "        # Plot 6: Slope distribution\n",
        "        print(\"\\n6. Creating Slope distribution plot...\")\n",
        "        if 'Slope [N/m]' in combined_df.columns:\n",
        "            slope_data = combined_df['Slope [N/m]'].dropna()\n",
        "            mean_val = slope_data.mean()\n",
        "            std_val = slope_data.std()\n",
        "\n",
        "            fig6, ax6 = plt.subplots(figsize=(10, 6))\n",
        "            n, bins, patches = ax6.hist(slope_data, bins=30, alpha=0.7, color='crimson',\n",
        "                                       edgecolor='black', linewidth=0.5)\n",
        "\n",
        "            ax6.axvline(mean_val, color='red', linestyle='--', linewidth=2,\n",
        "                       label=f'Mean: {mean_val:.3f} N/m')\n",
        "            ax6.axvline(mean_val + std_val, color='orange', linestyle=':', linewidth=1.5)\n",
        "            ax6.axvline(mean_val - std_val, color='orange', linestyle=':', linewidth=1.5,\n",
        "                       label=f'\\u00b11 SD: {std_val:.3f} N/m')\n",
        "\n",
        "            ax6.set_xlabel('Slope (N/m)', fontsize=12)\n",
        "            ax6.set_ylabel('Frequency', fontsize=12)\n",
        "            ax6.set_title(f'{project_name} - Slope Distribution\\n'\n",
        "                         f'N = {len(slope_data)} curves | Mean \\u00b1 SD: {mean_val:.3f} \\u00b1 {std_val:.3f} N/m',\n",
        "                         fontsize=14, fontweight='bold')\n",
        "            ax6.grid(True, alpha=0.3)\n",
        "            ax6.legend()\n",
        "            plt.tight_layout()\n",
        "\n",
        "            # Save individual plot to output directory\n",
        "            plot6_filename = f\"{project_name}_Slope_Distribution_{timestamp}.png\"\n",
        "            plot6_path = os.path.join(output_directory, plot6_filename)\n",
        "            plt.savefig(plot6_path, dpi=300, bbox_inches='tight')\n",
        "            plt.show()\n",
        "            print(f\"   ‚úÖ Saved as: {plot6_path}\")\n",
        "            plot_files.append(plot6_path)\n",
        "        else:\n",
        "            print(\"   ‚ö†Ô∏è  Slope data not found in the dataset\")\n",
        "\n",
        "        # Plot 7: Combined overview plot (bonus)\n",
        "        print(\"\\n7. Creating Combined Overview plot...\")\n",
        "        fig7, ((ax7a, ax7b), (ax7c, ax7d)) = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "        # Subplot 1: Young's Modulus\n",
        "        if 'Young\\'s Modulus [Pa]' in combined_df.columns:\n",
        "            youngs_mod_kpa = combined_df['Young\\'s Modulus [Pa]'].dropna() / 1000\n",
        "            ax7a.hist(youngs_mod_kpa, bins=25, alpha=0.7, color='steelblue', edgecolor='black')\n",
        "            ax7a.set_xlabel('Young\\'s Modulus (kPa)')\n",
        "            ax7a.set_ylabel('Frequency')\n",
        "            ax7a.set_title('Young\\'s Modulus')\n",
        "            ax7a.grid(True, alpha=0.3)\n",
        "\n",
        "        # Subplot 2: Adhesion\n",
        "        if 'Adhesion' in combined_df.columns:\n",
        "            adhesion_nN = combined_df['Adhesion'].dropna() * 1e9\n",
        "            ax7b.hist(adhesion_nN, bins=25, alpha=0.7, color='forestgreen', edgecolor='black')\n",
        "            ax7b.set_xlabel('Adhesion (nN)')\n",
        "            ax7b.set_ylabel('Frequency')\n",
        "            ax7b.set_title('Adhesion Force')\n",
        "            ax7b.grid(True, alpha=0.3)\n",
        "\n",
        "        # Subplot 3: Contact Point\n",
        "        if contact_cols:\n",
        "            contact_data = combined_df[contact_cols[0]].dropna() * 1e6\n",
        "            ax7c.hist(contact_data, bins=25, alpha=0.7, color='darkorange', edgecolor='black')\n",
        "            ax7c.set_xlabel('Contact Point (¬µm)')\n",
        "            ax7c.set_ylabel('Frequency')\n",
        "            ax7c.set_title('Contact Point')\n",
        "            ax7c.grid(True, alpha=0.3)\n",
        "\n",
        "        # Subplot 4: Slope\n",
        "        if 'Slope [N/m]' in combined_df.columns:\n",
        "            slope_data = combined_df['Slope [N/m]'].dropna()\n",
        "            ax7d.hist(slope_data, bins=25, alpha=0.7, color='crimson', edgecolor='black')\n",
        "            ax7d.set_xlabel('Slope (N/m)')\n",
        "            ax7d.set_ylabel('Frequency')\n",
        "            ax7d.set_title('Slope')\n",
        "            ax7d.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.suptitle(f'{project_name} - AFM Force Spectroscopy Summary\\nTotal Curves: {len(combined_df)}',\n",
        "                    fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "\n",
        "        # Save combined plot to output directory\n",
        "        plot7_filename = f\"{project_name}_Combined_Overview_{timestamp}.png\"\n",
        "        plot7_path = os.path.join(output_directory, plot7_filename)\n",
        "        plt.savefig(plot7_path, dpi=300, bbox_inches='tight')\n",
        "        plt.show()\n",
        "        print(f\"   ‚úÖ Saved as: {plot7_path}\")\n",
        "        plot_files.append(plot7_path)\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(f\"‚úÖ CREATED {len(plot_files)} PLOTS FOR '{project_name}'\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        # Download all plot files\n",
        "        print(\"\\nüì• DOWNLOADING ALL PLOT FILES...\")\n",
        "        for plot_file in plot_files:\n",
        "            if os.path.exists(plot_file):\n",
        "                files.download(plot_file)\n",
        "                print(f\"   Downloaded: {plot_file}\")\n",
        "\n",
        "        return plot_files\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Could not create plots: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "# Create plots if data is available\n",
        "if 'output_file' in locals() and output_file and os.path.exists(output_file):\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üìä CREATING VISUALIZATIONS...\")\n",
        "    print(\"=\" * 60)\n",
        "    plot_files = create_individual_plots(output_file, project_name, output_dir)\n",
        "    if plot_files:\n",
        "        print(f\"\\nüéâ All plots have been created and downloaded!\")\n",
        "        print(f\"   Look for files starting with: {project_name}_*.png\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 11. Optional: Create a ZIP file with all results (Saved in Input Directory)\n",
        "\n",
        "# %%\n",
        "def create_results_zip(project_name, output_directory):\n",
        "    \"\"\"Create a ZIP file containing all output files\"\"\"\n",
        "    try:\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        zip_filename = f\"{project_name}_AFM_Complete_Results_{timestamp}.zip\"\n",
        "        zip_path = os.path.join(output_directory, zip_filename)\n",
        "\n",
        "        # Find all files related to this project in the output directory\n",
        "        all_files = []\n",
        "\n",
        "        # Add Excel file\n",
        "        excel_files = glob.glob(os.path.join(output_directory, f\"{project_name}_AFM_Results_*.xlsx\"))\n",
        "        all_files.extend(excel_files)\n",
        "\n",
        "        # Add plot files\n",
        "        plot_files = glob.glob(os.path.join(output_directory, f\"{project_name}_*.png\"))\n",
        "        all_files.extend(plot_files)\n",
        "\n",
        "        if not all_files:\n",
        "            print(f\"‚ö†Ô∏è  No output files found for project '{project_name}' in {output_directory}\")\n",
        "            return None\n",
        "\n",
        "        # Create ZIP file\n",
        "        with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "            for file in all_files:\n",
        "                if os.path.exists(file):\n",
        "                    # Store files with their basename (without directory) in the zip\n",
        "                    zipf.write(file, os.path.basename(file))\n",
        "\n",
        "        print(f\"\\nüì¶ Created ZIP file: {zip_path}\")\n",
        "        print(f\"   Contains {len(all_files)} files:\")\n",
        "        for file in all_files:\n",
        "            print(f\"   ‚Ä¢ {os.path.basename(file)}\")\n",
        "\n",
        "        # Download the ZIP file\n",
        "        files.download(zip_path)\n",
        "\n",
        "        return zip_path\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error creating ZIP file: {e}\")\n",
        "        return None\n",
        "\n",
        "# Optional: Create ZIP file\n",
        "if 'project_name' in locals() and project_name:\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"üì¶ OPTIONAL: CREATE COMPLETE RESULTS PACKAGE\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    create_zip = input(\"Create a ZIP file with all results? (yes/no): \").strip().lower()\n",
        "\n",
        "    if create_zip == 'yes':\n",
        "        zip_file = create_results_zip(project_name, output_dir)\n",
        "        if zip_file:\n",
        "            print(f\"\\n‚úÖ Complete results package ready!\")\n",
        "    else:\n",
        "        print(\"Skipping ZIP creation.\")\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 12. Show Directory Contents\n",
        "\n",
        "# %%\n",
        "def show_directory_contents(directory):\n",
        "    \"\"\"Show contents of the directory\"\"\"\n",
        "    print(f\"\\nüìÅ CONTENTS OF DIRECTORY: {directory}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    if os.path.exists(directory):\n",
        "        items = os.listdir(directory)\n",
        "        if items:\n",
        "            # Separate files and directories\n",
        "            files = []\n",
        "            dirs = []\n",
        "\n",
        "            for item in items:\n",
        "                item_path = os.path.join(directory, item)\n",
        "                if os.path.isfile(item_path):\n",
        "                    files.append(item_path)\n",
        "                else:\n",
        "                    dirs.append(item_path)\n",
        "\n",
        "            # Show directories first\n",
        "            if dirs:\n",
        "                print(\"üìÇ DIRECTORIES:\")\n",
        "                for i, dir_path in enumerate(sorted(dirs), 1):\n",
        "                    dir_name = os.path.basename(dir_path)\n",
        "                    print(f\"  {i:3d}. üìÅ {dir_name}/\")\n",
        "\n",
        "            # Show files\n",
        "            if files:\n",
        "                print(\"\\nüìÑ FILES:\")\n",
        "                for i, filepath in enumerate(sorted(files), 1):\n",
        "                    filename = os.path.basename(filepath)\n",
        "                    size_bytes = os.path.getsize(filepath)\n",
        "\n",
        "                    # Convert size to appropriate unit\n",
        "                    if size_bytes < 1024:\n",
        "                        size_str = f\"{size_bytes} B\"\n",
        "                    elif size_bytes < 1024*1024:\n",
        "                        size_str = f\"{size_bytes/1024:.1f} KB\"\n",
        "                    elif size_bytes < 1024*1024*1024:\n",
        "                        size_str = f\"{size_bytes/(1024*1024):.1f} MB\"\n",
        "                    else:\n",
        "                        size_str = f\"{size_bytes/(1024*1024*1024):.1f} GB\"\n",
        "\n",
        "                    # Color code by file type\n",
        "                    if filename.endswith('.xlsx'):\n",
        "                        icon = \"üìä\"\n",
        "                    elif filename.endswith('.png'):\n",
        "                        icon = \"üñºÔ∏è\"\n",
        "                    elif filename.endswith('.zip'):\n",
        "                        icon = \"üì¶\"\n",
        "                    elif filename.endswith('.tsv') or filename.endswith('.txt'):\n",
        "                        icon = \"üìÑ\"\n",
        "                    else:\n",
        "                        icon = \"üìÑ\"\n",
        "\n",
        "                    print(f\"  {i:3d}. {icon} {filename:<50} ({size_str})\")\n",
        "        else:\n",
        "            print(\"   (Empty directory)\")\n",
        "    else:\n",
        "        print(\"   (Directory does not exist)\")\n",
        "\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "# Show contents of the output directory\n",
        "if 'output_dir' in locals():\n",
        "    show_directory_contents(output_dir)\n",
        "\n",
        "# %% [markdown]\n",
        "# ## 13. CLEANUP OPTION (Manual)\n",
        "\n",
        "# %%\n",
        "def manual_cleanup():\n",
        "    \"\"\"Manual cleanup function if needed\"\"\"\n",
        "    print(\"üîÑ MANUAL CLEANUP OPTION\")\n",
        "    print(\"-\" * 40)\n",
        "    print(\"This will remove all generated files and start fresh.\")\n",
        "    print(\"Files to be removed:\")\n",
        "\n",
        "    files_to_remove = []\n",
        "    for pattern in ['*.xlsx', '*.png', '*.pdf', 'AFM_*', f'{project_name}_*', 'Hydrogel_*']:\n",
        "        files_to_remove.extend(glob.glob(pattern))\n",
        "\n",
        "    if files_to_remove:\n",
        "        for filepath in files_to_remove:\n",
        "            print(f\"  ‚Ä¢ {os.path.basename(filepath)}\")\n",
        "\n",
        "        confirm = input(\"\\nAre you sure? (yes/no): \")\n",
        "        if confirm.lower() == 'yes':\n",
        "            for filepath in files_to_remove:\n",
        "                try:\n",
        "                    os.remove(filepath)\n",
        "                except:\n",
        "                    pass\n",
        "            print(\"‚úÖ Cleanup complete!\")\n",
        "        else:\n",
        "            print(\"‚ùå Cleanup cancelled.\")\n",
        "    else:\n",
        "        print(\"‚úÖ No files to clean up.\")\n",
        "\n",
        "    # Also clean directories\n",
        "    for dir_path in [f'uploaded_files_{project_name}', 'uploaded_files']:\n",
        "        if os.path.exists(dir_path):\n",
        "            try:\n",
        "                shutil.rmtree(dir_path)\n",
        "                print(f\"‚úÖ Removed directory: {dir_path}\")\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "# Uncomment to enable manual cleanup button\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"To manually clean up files and start over, run:\")\n",
        "print(\"manual_cleanup()\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üéâ ANALYSIS COMPLETE!\")\n",
        "print(f\"All results for '{project_name}' have been generated and saved to:\")\n",
        "print(f\"  {output_dir}\")\n",
        "print(\"=\" * 60)"
      ]
    }
  ]
}